{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cdbd2f-a164-4f9e-a5ed-a430f838ddd6",
   "metadata": {},
   "source": [
    "<h1> Image detection comparasion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f819b-5984-4100-be01-cff8dd805b1a",
   "metadata": {},
   "source": [
    "<h3> Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7107c6c0-f0bf-4764-b00a-d7ed4d3bfa5a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92fea2c-41d4-4198-8d43-ea0f25201ac9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5328118-40e7-45fc-b93b-b23b6d5bb0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "IMG_WIDTH =50\n",
    "IMG_HEIGHT =50\n",
    "NUM_CATEGORIES = 5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c80430-f306-4fde-bda8-e46146170b0a",
   "metadata": {},
   "source": [
    "<h3> Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae98e6-0f30-4072-8111-8e296ff1ed8b",
   "metadata": {},
   "source": [
    "Data will be imported as first. It is divided directly in the training set and test set as it is already divided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de24b28b-32e7-4c51-87c0-5f154dd76fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_np(path, size_width = IMG_WIDTH, size_height =  IMG_HEIGHT):\n",
    "    images = list()\n",
    "    labels = list()\n",
    "\n",
    "    complete_path = os.getcwd() + path\n",
    "    for folder in os.scandir(complete_path):\n",
    "        #print(f\"Scanninng folder {folder.name}\")\n",
    "        label = folder.name\n",
    "        print(f\"Reading folder: {folder.name}\")\n",
    "        for file in os.scandir(folder):\n",
    "            if file.name == '.ipynb_checkpoints': continue\n",
    "            # Reading each image\n",
    "            image = cv2.imread(file.path)\n",
    "            if image is None: print(f\"Reading image {complete_path + file.name} was not possible\")\n",
    "\n",
    "            #Resizing the image to IMG_WIDTH, IMG_HEIGHT\n",
    "            image = cv2.resize(image,(size_width,size_height))\n",
    "\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "            print(f\"Total images loaded: {len(labels)}\", end='\\r')\n",
    "        print('\\n')\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff23da72-e7f2-43b3-96d6-b9ec25683aff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder: cat\n",
      "Total images loaded: 2737\n",
      "\n",
      "Reading folder: dog\n",
      "Total images loaded: 5364\n",
      "\n",
      "Reading folder: elephant\n",
      "Total images loaded: 8094\n",
      "\n",
      "Reading folder: horse\n",
      "Total images loaded: 10799\n",
      "\n",
      "Reading folder: lion\n",
      "Total images loaded: 13474\n",
      "\n",
      "Reading folder: cat\n",
      "Total images loaded: 300\n",
      "\n",
      "Reading folder: dog\n",
      "Total images loaded: 600\n",
      "\n",
      "Reading folder: elephant\n",
      "Total images loaded: 899\n",
      "\n",
      "Reading folder: horse\n",
      "Total images loaded: 1199\n",
      "\n",
      "Reading folder: lion\n",
      "Total images loaded: 1497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_train, labels_train_categories = get_data_np('/animals/train/')\n",
    "images_test, labels_test_categories = get_data_np('/animals/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d270edfa-6a56-4b08-ba7f-a6e0831d0fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'cat', 'cat', ..., 'lion', 'lion', 'lion'], dtype='<U8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3850708e-ed9c-4594-9d79-7daf81aea34d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'cat', 'cat', ..., 'lion', 'lion', 'lion'], dtype='<U8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098f90c-c782-4f64-8223-1e42434d5d49",
   "metadata": {},
   "source": [
    "Let's now create a dictionary to map for each catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee786f04-1673-4964-955b-037051584b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'dog', 'elephant', 'horse', 'lion'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b1bd09-09de-45f4-afbf-f9c948840be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_categories = dict()\n",
    "for i, label in enumerate(np.unique(labels_train_categories)):\n",
    "    mapping_categories[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c21d3e3-d7d5-4412-bf94-461bdf58d329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1, 'elephant': 2, 'horse': 3, 'lion': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2549c8-36e8-44bc-b1d5-e985a40a2541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_labels(labels):\n",
    "    labels_categories = np.empty(labels.shape)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        labels_categories[i] = mapping_categories[label]\n",
    "    \n",
    "    return labels_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f55a41-b448-4095-9a7c-79efd49ed8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_train = map_labels(labels_train_categories)\n",
    "labels_test = map_labels(labels_test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b776e3-33ce-4c76-aaa5-4078fe4ab5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 4., 4., 4.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3dd5723-220b-404b-95ad-15ab0cc68dee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 4., 4., 4.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eb6df-08c5-4f0d-a952-7d73ab39e9d9",
   "metadata": {},
   "source": [
    "<h3> Simple Model: Convolution with One layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7627e4e-8346-4ed1-8baa-1cd443d89ff9",
   "metadata": {},
   "source": [
    "The first model to be built will be a simple convolutional netwcork with only a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "932c663d-6277-4deb-babb-d502292c9eac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 22:24:16.909902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-01-16 22:24:16.910170: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-01-16 22:24:16.910223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterlab-velascosergi): /proc/driver/nvidia/version does not exist\n",
      "2024-01-16 22:24:16.919416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model1 =  model = tf.keras.models.Sequential()\n",
    "\n",
    "#Create a Conv2D layer\n",
    "model1.add(tf.keras.layers.Conv2D(50, (3,3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "#Create a Pooling layer\n",
    "model1.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten layer\n",
    "model1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model1.add(tf.keras.layers.Dropout(0.05))\n",
    "\n",
    "#Output layer\n",
    "model1.add(tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\"))\n",
    "\n",
    "#Model compilation\n",
    "model1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb50af83-c9ac-46d4-85bd-0323f3a680f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 50)        1400      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 50)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 28800)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 144005    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,405\n",
      "Trainable params: 145,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2d65ec7-6b3e-481b-bc72-5a227559e7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 31s 71ms/step - loss: 12.2161 - accuracy: 0.4716\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.8931 - accuracy: 0.6737\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.6574 - accuracy: 0.7569\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.5358 - accuracy: 0.8072\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.4682 - accuracy: 0.8321\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 29s 69ms/step - loss: 0.4020 - accuracy: 0.8541\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.4272 - accuracy: 0.8549\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 29s 69ms/step - loss: 0.3691 - accuracy: 0.8743\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 31s 73ms/step - loss: 0.3591 - accuracy: 0.8745\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 29s 69ms/step - loss: 0.3681 - accuracy: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e69814590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(images_train, labels_train, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b67afc-bab8-4d52-b6ce-b92fb2526053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 12ms/step - loss: 1.6268 - accuracy: 0.6860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6267576217651367, 0.6860387325286865]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0176fdf0-40d6-408c-b488-82c8f939464b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model1/assets\n"
     ]
    }
   ],
   "source": [
    "model1.save(\"Model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecef124-9b0a-4325-bf72-d3dbe47fdfe8",
   "metadata": {},
   "source": [
    "<h3> Complex model: 3 convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202df9e-ef6e-406c-a034-ecddc688b9df",
   "metadata": {},
   "source": [
    "Now we will use the basic model to add two internal layers and evaluate the new performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d84c07-c00a-4bf1-86bd-47d11bd51860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential()\n",
    "\n",
    "#Create a Conv2D layer\n",
    "model2.add(tf.keras.layers.Conv2D(50, (3,3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "#Create a Pooling layer\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    " #2nd layer\n",
    "model2.add(tf.keras.layers.Conv2D(50, (2,2), activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#3rd layer\n",
    "model2.add(tf.keras.layers.Conv2D(50, (1,1), activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten layer\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model2.add(tf.keras.layers.Dropout(0.05))\n",
    "\n",
    "#Output layer\n",
    "model2.add(tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\"))\n",
    "\n",
    "#Model compilation\n",
    "model2.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "171e30b3-e515-4a2d-a795-b7f02ec44987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 50)        1400      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 50)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 23, 23, 50)        10050     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 11, 11, 50)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 50)        2550      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1250)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1250)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 6255      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,255\n",
      "Trainable params: 20,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd3c8231-a868-4844-8f22-3ec8f3aba0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 32s 74ms/step - loss: 2.1480 - accuracy: 0.4682\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 30s 70ms/step - loss: 0.8766 - accuracy: 0.6730\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 31s 73ms/step - loss: 0.7315 - accuracy: 0.7252\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 30s 70ms/step - loss: 0.6741 - accuracy: 0.7402\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 31s 75ms/step - loss: 0.6195 - accuracy: 0.7664\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 30s 70ms/step - loss: 0.5782 - accuracy: 0.7814\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.5588 - accuracy: 0.7889\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 30s 72ms/step - loss: 0.5287 - accuracy: 0.8031\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 31s 73ms/step - loss: 0.5078 - accuracy: 0.8007\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.4826 - accuracy: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e60539f10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(images_train, labels_train, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a6f0ed-a097-40a1-b0cf-474d9a092bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 18ms/step - loss: 0.5221 - accuracy: 0.7943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5221474766731262, 0.7942551970481873]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b81ac94-c2ea-486f-8211-7563a5660139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model2/assets\n"
     ]
    }
   ],
   "source": [
    "model2.save(\"Model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51252c5-e327-4d94-ad1e-a6d31f99dc65",
   "metadata": {},
   "source": [
    "<h3> Loading ResNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448a4204-7156-4d05-a61c-aec6c28b695d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e0577d9-31db-43da-a441-9fd26d953016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#del basemodel, x, model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0cf7bbd-0ec9-448d-9bd1-a1ea0287cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# initialize the base model\n",
    "basemodel = ResNet50(input_shape=(IMG_WIDTH,IMG_HEIGHT,3),\n",
    "                          include_top = False,\n",
    "                          weights = 'imagenet')\n",
    "\n",
    "for layer in basemodel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#x = basemodel.output\n",
    "    \n",
    "#x = tf.keras.models.Sequential()\n",
    "x = tf.keras.layers.Flatten()(basemodel.output)\n",
    "x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.05)(x)\n",
    "\n",
    "#Output layer\n",
    "output_layer = tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")(x)\n",
    "\n",
    "model3 = Model(basemodel.input,output_layer)\n",
    "\n",
    "model3.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26e538-99ab-4708-a077-8527eb26e4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687b0dc0-2b2d-4648-9b63-5362f136e172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13474/13474 [==============================] - 536s 40ms/sample - loss: 0.3592 - acc: 0.8634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc500a9850>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(images_train, labels_train, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "569ec33f-93e8-4cdc-b66c-b05c5d1c60fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497/1497 [==============================] - 48s 32ms/sample - loss: 1.0005 - acc: 0.6907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.000512338990598, 0.6907148]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ed9c022-a15f-4671-bb1e-de3524ecdb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.save('Model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997ec94-8019-449e-9d32-d638f0289fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
